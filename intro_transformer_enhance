from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

def create_simple_llm():
  """
  Creates a simpll LLM using a small GPT-2 model.
  GPT-2 (smallest version) is perfect for demonstrations as it's:
  - Relatively small (124M parameters)
  - Fast enough to run on CPU
  - Good for understanding basic concepts
  """
  # Initialize the model and tokenizer
  model_name = "distilgpt2"
  
  # Create the generator pipline
  generator = pipeline('text-generation',
                       model=model_name,
                       pad_token_id=50256)
  
  return generator


def generate_text(generator, prompt, max_length=1000):
  # Generate text
  
  
  result = gerator(prompt, max_length, num_return_sequences=1, do_sample=True, temperature=0.7)
  return result[0]["generated_text"]


def run_llm_demo():
  """
  Demonstrates basic LLM functionality with explanations
  """
  print(":-) Loading Simple LLM Model...")
  generator = create_simple_llm()
  
  print("\n Simple LLM Demo ")
  